# Deep Q-Network

Reinforcement learning (RL) is a type of machine learning process that focuses on decision making by autonomous agents. An autonomous agent is any system that can make decisions and act in response to its environment independent of direct instruction by a human user. 

Reinforcement learning essentially consists of the relationship between an agent, environment, and goal. Literature widely formulates this relationship in terms of the Markov decision process (MDP).

The RL agent learns about a problem by interacting with its environment. The environment provides information on its current state. The agent then uses that information to determine which action(s) to take. If that action obtains a reward signal from the surrounding environment, the agent is encouraged to take that action again when in a similar future state. This process repeats for every new state thereafter. Over time, the agent learns from rewards and punishments to take actions within the environment that meet a specified goal. In MDP, state space refers to all of the information provided by an environment’s state. Action space denotes all possible actions the agent may take within a state.

A RL agent has to explore its environment, attempting new actions to discover those that receive rewards. From these reward signals, the agent learns to prefer actions for which it was rewarded in order to maximize its gain. But the agent must continue exploring new states and actions as well. RL algorithms thus require an agent to both exploit knowledge of previously rewarded state-actions and explore other state-actions.

Dynamic programming breaks down larger tasks into smaller tasks. Thus, it models problems as workflows of sequential decision made at discrete time steps. Each decision is made in terms of the resulting possible next state. An agent’s reward (r) for a given action is defined as a function of that action (a), the current environmental state (s), and the potential next state (s’)

This reward function can be used as (part of) the policy governing an agent’s actions. Determining the optimal policy for agent behavior is a chief component of dynamic programming methods for RL. Enter the Bellman equation: in short, this equation defines vt(s) as the total expected reward starting at time t until the end of a decision workflow. It assumes that the agent begins by occupying state s at time t. The equation ultimately divides the reward at time t into the immediate reward rt(s,a) and the agent’s total expected reward. An agent thus maximizes its value function, being the total value of the Bellman equation, by consistently choosing that action which receives a reward signal in each state.

Q-Learning is a model-free RL algorithm designed to help an agent learn the optimal actions to take in a given state to maximize rewards over time. It doesn’t require prior knowledge of the environment’s dynamics (hence, “model-free”). Instead, it learns through trial and error, interacting with the environment and adjusting its behavior based on the feedback received. The core idea of Q-learning is that it learns to estimate the cumulative future rewards of taking a particular action in a given state. It does this by starting with estimates of immediate rewards and iteratively extending those short-term estimates to longer-term estimates. The essence of Q-Learning lies in the Q-table, a data structure that stores the estimated values (Q-values) of taking different actions in different states. These Q-values represent the expected future rewards for each action-state pair. 

Deep RL is using nonlinear function approximators to calculate the value actions based directly on observation from the environment. A Deep Neural Network is adopted to find the optimal parameters for these function approximators. The Deep Q-Learning algorithm represents the optimal action-value function q∗ as a neural network (instead of a table). Unfortunately, reinforcement learning is notoriously unstable when neural networks are used to represent the action values.

In 2015, DeepMind made a breakthrough by designing an agent that learned to play video games better than humans. They call this agent a Deep Q-Network, let's take a close look at how it works. True to its name, at the heart of the agent is a deep neural network that acts as a function approximator. The original DQN agent used three such convolutional layers with ReLU activation, regularized linear units. They were followed by one fully-connected hidden layer with ReLU activation, and one fully-connected linear output layer that produced the vector of action values. 
Training such a network requires a lot of data, but even then, it is not guaranteed to converge on the optimal value function. In fact, there are situations where the network weights can oscillate or diverge, due to the high correlation between actions and states. This can result in a very unstable and ineffective policy. In order to overcome these challenges, the researchers came up with several techniques that slightly modified the base Q-learning algorithm: 
-    Experience Replay: in summary, when the agent interacts with the environment, the sequence of experience tuples can be highly correlated. The naive Q-learning algorithm that learns from each of these experience tuples in sequential order runs the risk of getting swayed by the effects of this correlation. By instead keeping track of a replay buffer and using experience replay to sample from the buffer at random, it is possible to prevent action values from oscillating or diverging catastrophically.
-    Fixed Q-Targets: it consists in the use of fixed parameters indicated by a w-, they are basically a copy of w that it is not changed during the learning step. In practice, w is copied into w-, it is used to generate targets while changing w for a certain number of learning steps. Then, w- is updated  with the latest w, again, learn for a number of steps and so on. This decouples the target from the parameters, makes the learning algorithm much more stable, and less likely to diverge or fall into oscillations.

Several other techniques have subsequently enriched the Deep Q-Network, specifically, the following have been implemented in the code:
-    Double DQN: to address the the overestimation of action values that Q-learning is proneto, the best action is selected using one set of parameters w, but it is evaluated using a different set of parameters w' (when using DQNs with fixed Q-Targets, w' corresponds to w-). In my agents, this is implemented thanks to the `double_dqn_learn` functions in `dqn_agent.py`.
-    Dueling DQN: the core idea of this technique is to use two streams, one that estimates the state value function and one that estimates the advantage for each action.  Finally, the desired Q-values are obtained by combining the state and advantage values. This is implemented in `model.py` due to the class `DuelingQNetwork`.
-    Prioritized Experience Replay: it is implemented to address the fact that some experiences may be more important for learning than others; moreover, these important experiences might occur infrequently: sampling the batches uniformly, then these experiences have a very small chance of getting selected and, since buffers are practically limited in capacity, older important experiences may get lost. One criteria  to assign priorities to each tuple is to use the TD error delta. The bigger the error, the more we expect to learn from that tuple. So, the magnitude of this error can be adopted as a measure of priority and store it along with each corresponding tuple in the replay buffer. This causes the modification of the update rule. My implementation is in  `dqn_agent.py` thanks to the classes `PrioritizedExperienceReplayAgent` and `PrioritizedReplayBuffer` (obviously, it is possible to combine it with Double DQN and Dueling DQN).

In `Navigation.ipynb`, there is the possibility to train an agent, setting the use of Double DQN and Dueling DQN, and a Prioritized Experience Replay agent to, again setting the use of Double DQN and Dueling DQN. This distinction is due to the fact that Prioritized Experience Replay agent has some additional hyperparameters and behaviours, so that I decided to manage it separately.

As already specified in `README.md`, `checkpoint.pth` contains the saved model weights of a successful agent and `score_episode_plot.pth` shows the trend of the average score over episodes during the training phase of the same agent until the goal is reached (average score of +13 over 100 consecutive episodes). Specifically, this agent reaches the gol in 425 episodes and it is based on naive DQN, with the additional use of DuelingDQN and DoubleDQN techniques.

A possible future idea to improve agent performance is to combine all the extention (not only the three I considered) to the naive DQN to create the so called Rainbow Agent.






